{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms \n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 3\n",
    "NUM_ROUTING_ITERATIONS = 3\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, batch_size):\n",
    "        dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=dataset_transform)\n",
    "        test_dataset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=dataset_transform)\n",
    "        \n",
    "        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(v):\n",
    "    epsilon = 0.00000001\n",
    "    vector_norm = (v ** 2).sum(-1,keepdim = True) + epsilon\n",
    "    output = vector_norm * v/((1. + vector_norm) * torch.sqrt(vector_norm))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一层，使用普通卷积得到基础特征\n",
    "#(batch,28,28,1)\n",
    "class Convlayer(nn.Module):\n",
    "    def __init__(self,in_channels = 1,out_channels = 256,kernel_size = 9):\n",
    "        super(Convlayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride = 1)\n",
    "    def forward(self,x):\n",
    "        #(batch_size,20,20,256)\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCapslayer(nn.Module):\n",
    "    def __init__(self,num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n",
    "        super(PrimaryCapslayer,self).__init__()\n",
    "        self.capsules = nn.ModuleList([nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = kernel_size,stride = 2,padding=0)\n",
    "                                      for _ in range(num_capsules)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        u = [capsule(x).view(x.size(0),-1,1) for capsule in self.capsules]\n",
    "        #u:(batch_size,8,6,6,32)\n",
    "        u = torch.cat(u,dim=-1)\n",
    "        #u:(batch_size,1152,8)\n",
    "        return squash(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self,num_capsules = 10,num_routes = 32*6*6,in_channels = 8,out_channels = 16):\n",
    "        super(DigitCaps,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_routes = num_routes\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(num_routes,num_capsules,out_channels,in_channels))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        u = x.unsqueeze(3)\n",
    "        u = u.unsqueeze(2)\n",
    "        #print(\"u.shape:\",u.shape)\n",
    "        #x:(batch_size,1152,1,8,1)\n",
    "        #W:(1152,10,16,8)\n",
    "        #W*x = (batch_size,1152,10,16,1)\n",
    "        #W = self.W.unsqueeze(0)\n",
    "        #print(\"w.shape\",self.W.shape)\n",
    "        u_hat = torch.matmul(self.W,u)\n",
    "        #u_hat(batch_size, 1152, 10, 16, 1)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        #print(\"u_hat.shape\",u_hat.shape)\n",
    "        u_hat=u_hat.permute(0,2,1,3)\n",
    "        \n",
    "        b_ij = torch.zeros(u_hat.size(0),self.num_capsules,1,self.num_routes)\n",
    "        if USE_CUDA:\n",
    "            b_ij = b_ij.cuda()\n",
    "            \n",
    "        num_iterations = NUM_ROUTING_ITERATIONS\n",
    "        for iteration in range(num_iterations):\n",
    "            #print(b_ij.shape)\n",
    "            c_ij = F.softmax(b_ij,-1)\n",
    "            #print(\"u_hat.shape\",u_hat.shape)\n",
    "            #print(\"c_ij.shape:\",c_ij.shape)\n",
    "            s_j = torch.matmul(c_ij,u_hat)\n",
    "            #print(\"s_j:\",s_j.shape)\n",
    "            v_j = squash(s_j)\n",
    "            if iteration < num_iterations -1:\n",
    "                a_ij = torch.matmul(v_j,u_hat.permute(0,1,3,2))\n",
    "                b_ij = b_ij + a_ij\n",
    "        v_j = v_j.permute(0,1,3,2).squeeze(-1)\n",
    "        #print(\"v_j\",v_j.shape)\n",
    "        return v_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet,self).__init__()\n",
    "        self.conv_layer = Convlayer()\n",
    "        self.primarycaps_layer = PrimaryCapslayer()\n",
    "        self.DigitCaps_layer = DigitCaps()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,data):\n",
    "        data = self.conv_layer(data)\n",
    "        data = self.primarycaps_layer(data)\n",
    "        output = self.DigitCaps_layer(data)\n",
    "        return output\n",
    "    def loss(self, data, x, target):\n",
    "        return self.margin_loss(x, target) \n",
    "    \n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2))\n",
    "        #print(\"vc\",v_c.shape)\n",
    "        #(batch_size,10)\n",
    "        left = F.relu(0.9 - v_c)**2\n",
    "        right = F.relu(v_c - 0.1)**2\n",
    "        labels = torch.sparse.torch.eye(10).index_select(dim=0, index=labels.data.cpu())\n",
    "        #print(labels.shape)\n",
    "        if USE_CUDA:\n",
    "            labels = labels.cuda()\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        return loss * 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capsule_net = CapsNet()\n",
    "if USE_CUDA:\n",
    "    capsule_net = capsule_net.cuda()\n",
    "optimizer = optim.Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "0.13491101562976837\n",
      "train accuracy: 0.86\n",
      "0.09772814810276031\n",
      "train accuracy: 0.93\n",
      "0.07092873752117157\n",
      "train accuracy: 0.94\n",
      "0.0762912929058075\n",
      "train accuracy: 0.92\n",
      "0.06049880012869835\n",
      "train accuracy: 0.96\n",
      "0.054591625928878784\n",
      "train accuracy: 0.98\n",
      "0.037535686045885086\n",
      "train accuracy: 0.97\n",
      "0.021849989891052246\n",
      "train accuracy: 1.0\n",
      "0.052823055535554886\n",
      "train accuracy: 0.96\n",
      "0.03668174520134926\n",
      "train accuracy: 0.97\n",
      "0.04867825284600258\n",
      "train accuracy: 0.95\n",
      "0.0379539430141449\n",
      "train accuracy: 0.95\n",
      "0.02712220698595047\n",
      "train accuracy: 0.96\n",
      "0.02531808242201805\n",
      "train accuracy: 0.99\n",
      "0.027278419584035873\n",
      "train accuracy: 0.98\n",
      "0.03970300406217575\n",
      "train accuracy: 0.96\n",
      "0.03321731090545654\n",
      "train accuracy: 0.97\n",
      "0.03557023033499718\n",
      "train accuracy: 0.98\n",
      "0.023673653602600098\n",
      "train accuracy: 0.98\n",
      "0.03448548540472984\n",
      "train accuracy: 0.97\n",
      "0.025972655043005943\n",
      "train accuracy: 0.98\n",
      "0.023576343432068825\n",
      "train accuracy: 0.99\n",
      "0.04173723980784416\n",
      "train accuracy: 0.96\n",
      "0.011991513893008232\n",
      "train accuracy: 1.0\n",
      "0.023215658962726593\n",
      "train accuracy: 0.98\n",
      "0.03364500403404236\n",
      "train accuracy: 0.95\n",
      "0.042240314185619354\n",
      "train accuracy: 0.97\n",
      "0.028957214206457138\n",
      "train accuracy: 0.95\n",
      "0.01095906924456358\n",
      "train accuracy: 1.0\n",
      "0.019856659695506096\n",
      "train accuracy: 0.98\n",
      "0.02412925846874714\n",
      "train accuracy: 0.98\n",
      "0.012061187997460365\n",
      "train accuracy: 0.99\n",
      "0.01000983826816082\n",
      "train accuracy: 0.99\n",
      "0.01816967874765396\n",
      "train accuracy: 0.98\n",
      "0.03674997016787529\n",
      "train accuracy: 0.94\n",
      "0.021811779588460922\n",
      "train accuracy: 0.98\n",
      "0.03212377801537514\n",
      "train accuracy: 0.98\n",
      "0.0165223628282547\n",
      "train accuracy: 0.98\n",
      "0.01089557260274887\n",
      "train accuracy: 0.99\n",
      "0.013272368349134922\n",
      "train accuracy: 1.0\n",
      "0.009712898172438145\n",
      "train accuracy: 1.0\n",
      "0.016187062487006187\n",
      "train accuracy: 0.98\n",
      "0.011696968227624893\n",
      "train accuracy: 0.99\n",
      "0.017732329666614532\n",
      "train accuracy: 0.98\n",
      "0.007619152311235666\n",
      "train accuracy: 1.0\n",
      "0.014496655203402042\n",
      "train accuracy: 0.99\n",
      "0.0404973030090332\n",
      "train accuracy: 0.96\n",
      "0.025516990572214127\n",
      "train accuracy: 0.97\n",
      "0.02005680464208126\n",
      "train accuracy: 0.97\n",
      "0.011147929355502129\n",
      "train accuracy: 0.99\n",
      "0.025079848244786263\n",
      "train accuracy: 0.97\n",
      "0.018874917179346085\n",
      "train accuracy: 0.98\n",
      "0.010376950725913048\n",
      "train accuracy: 0.99\n",
      "0.031400229781866074\n",
      "train accuracy: 0.98\n",
      "0.006697141565382481\n",
      "train accuracy: 1.0\n",
      "0.00964111927896738\n",
      "train accuracy: 0.99\n",
      "0.032685019075870514\n",
      "train accuracy: 0.96\n",
      "0.0135890431702137\n",
      "train accuracy: 1.0\n",
      "0.011317329481244087\n",
      "train accuracy: 1.0\n",
      "0.004395242780447006\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.96\n",
      "0.013600885223131626\n",
      "0.006333281751722097\n",
      "train accuracy: 1.0\n",
      "0.006612114608287811\n",
      "train accuracy: 1.0\n",
      "0.011785604991018772\n",
      "train accuracy: 0.99\n",
      "0.0043419585563242435\n",
      "train accuracy: 1.0\n",
      "0.008920484222471714\n",
      "train accuracy: 0.99\n",
      "0.013971894979476929\n",
      "train accuracy: 0.97\n",
      "0.004282328300178051\n",
      "train accuracy: 1.0\n",
      "0.004779420793056488\n",
      "train accuracy: 1.0\n",
      "0.012192530557513237\n",
      "train accuracy: 0.99\n",
      "0.028015360236167908\n",
      "train accuracy: 0.97\n",
      "0.009493380784988403\n",
      "train accuracy: 1.0\n",
      "0.01318898145109415\n",
      "train accuracy: 0.98\n",
      "0.01772163435816765\n",
      "train accuracy: 0.98\n",
      "0.01073148101568222\n",
      "train accuracy: 0.98\n",
      "0.003314538858830929\n",
      "train accuracy: 1.0\n",
      "0.020851556211709976\n",
      "train accuracy: 0.98\n",
      "0.0104653500020504\n",
      "train accuracy: 1.0\n",
      "0.009205546230077744\n",
      "train accuracy: 0.99\n",
      "0.015078038908541203\n",
      "train accuracy: 0.98\n",
      "0.006343569606542587\n",
      "train accuracy: 1.0\n",
      "0.010312252677977085\n",
      "train accuracy: 1.0\n",
      "0.004281600471585989\n",
      "train accuracy: 1.0\n",
      "0.02172640711069107\n",
      "train accuracy: 0.97\n",
      "0.011788425967097282\n",
      "train accuracy: 0.99\n",
      "0.007743282709270716\n",
      "train accuracy: 0.99\n",
      "0.0037667283322662115\n",
      "train accuracy: 1.0\n",
      "0.004258743952959776\n",
      "train accuracy: 1.0\n",
      "0.017078179866075516\n",
      "train accuracy: 0.99\n",
      "0.012866237200796604\n",
      "train accuracy: 0.99\n",
      "0.005568109452724457\n",
      "train accuracy: 1.0\n",
      "0.013881630264222622\n",
      "train accuracy: 0.98\n",
      "0.00829063355922699\n",
      "train accuracy: 1.0\n",
      "0.013282057829201221\n",
      "train accuracy: 0.99\n",
      "0.010544953867793083\n",
      "train accuracy: 0.99\n",
      "0.011063744314014912\n",
      "train accuracy: 0.99\n",
      "0.007674042601138353\n",
      "train accuracy: 1.0\n",
      "0.006392503622919321\n",
      "train accuracy: 0.99\n",
      "0.0068572163581848145\n",
      "train accuracy: 1.0\n",
      "0.009836828336119652\n",
      "train accuracy: 0.99\n",
      "0.014236075803637505\n",
      "train accuracy: 0.99\n",
      "0.018093673512339592\n",
      "train accuracy: 0.97\n",
      "0.015088271349668503\n",
      "train accuracy: 0.99\n",
      "0.01774611510336399\n",
      "train accuracy: 0.98\n",
      "0.004731924273073673\n",
      "train accuracy: 1.0\n",
      "0.011930410750210285\n",
      "train accuracy: 0.99\n",
      "0.003118305467069149\n",
      "train accuracy: 1.0\n",
      "0.0027495413087308407\n",
      "train accuracy: 1.0\n",
      "0.0025465544313192368\n",
      "train accuracy: 1.0\n",
      "0.012335995212197304\n",
      "train accuracy: 0.98\n",
      "0.023138482123613358\n",
      "train accuracy: 0.96\n",
      "0.015921983867883682\n",
      "train accuracy: 0.99\n",
      "0.009912480600178242\n",
      "train accuracy: 0.99\n",
      "0.012666513212025166\n",
      "train accuracy: 0.99\n",
      "0.03279775753617287\n",
      "train accuracy: 0.97\n",
      "0.0072927032597362995\n",
      "train accuracy: 1.0\n",
      "0.021396033465862274\n",
      "train accuracy: 0.97\n",
      "0.00886045303195715\n",
      "train accuracy: 0.99\n",
      "0.007107330486178398\n",
      "train accuracy: 1.0\n",
      "0.008270529098808765\n",
      "train accuracy: 0.99\n",
      "0.016606075689196587\n",
      "train accuracy: 0.98\n",
      "test accuracy: 1.0\n",
      "0.01057219021487981\n",
      "0.00887187197804451\n",
      "train accuracy: 1.0\n",
      "0.0037869582884013653\n",
      "train accuracy: 1.0\n",
      "0.0072441850788891315\n",
      "train accuracy: 0.99\n",
      "0.005642099305987358\n",
      "train accuracy: 1.0\n",
      "0.007970038801431656\n",
      "train accuracy: 1.0\n",
      "0.010509895160794258\n",
      "train accuracy: 0.99\n",
      "0.007680519483983517\n",
      "train accuracy: 0.99\n",
      "0.006764090619981289\n",
      "train accuracy: 0.99\n",
      "0.013067175634205341\n",
      "train accuracy: 0.98\n",
      "0.007077854126691818\n",
      "train accuracy: 1.0\n",
      "0.005570997949689627\n",
      "train accuracy: 1.0\n",
      "0.008499634452164173\n",
      "train accuracy: 0.99\n",
      "0.006716859061270952\n",
      "train accuracy: 1.0\n",
      "0.005120490677654743\n",
      "train accuracy: 1.0\n",
      "0.0029666863847523928\n",
      "train accuracy: 1.0\n",
      "0.00777223939076066\n",
      "train accuracy: 0.99\n",
      "0.008422845974564552\n",
      "train accuracy: 1.0\n",
      "0.0034141247160732746\n",
      "train accuracy: 1.0\n",
      "0.0010913118021562696\n",
      "train accuracy: 1.0\n",
      "0.004171472042798996\n",
      "train accuracy: 1.0\n",
      "0.0021239840425550938\n",
      "train accuracy: 1.0\n",
      "0.007696439512073994\n",
      "train accuracy: 1.0\n",
      "0.004491542465984821\n",
      "train accuracy: 1.0\n",
      "0.009118017740547657\n",
      "train accuracy: 1.0\n",
      "0.0009357993840239942\n",
      "train accuracy: 1.0\n",
      "0.0038762458134442568\n",
      "train accuracy: 1.0\n",
      "0.01860881596803665\n",
      "train accuracy: 0.97\n",
      "0.0021246497053653\n",
      "train accuracy: 1.0\n",
      "0.006014077924191952\n",
      "train accuracy: 0.99\n",
      "0.006819700822234154\n",
      "train accuracy: 1.0\n",
      "0.0070325457490980625\n",
      "train accuracy: 0.99\n",
      "0.0016411106334999204\n",
      "train accuracy: 1.0\n",
      "0.006241192575544119\n",
      "train accuracy: 1.0\n",
      "0.008985821157693863\n",
      "train accuracy: 0.99\n",
      "0.004035166930407286\n",
      "train accuracy: 1.0\n",
      "0.0055894809775054455\n",
      "train accuracy: 1.0\n",
      "0.013520939275622368\n",
      "train accuracy: 0.98\n",
      "0.01061369851231575\n",
      "train accuracy: 0.99\n",
      "0.0035798188764601946\n",
      "train accuracy: 1.0\n",
      "0.012297887355089188\n",
      "train accuracy: 0.99\n",
      "0.00553930876776576\n",
      "train accuracy: 1.0\n",
      "0.007343987002968788\n",
      "train accuracy: 1.0\n",
      "0.006324416026473045\n",
      "train accuracy: 0.99\n",
      "0.006985601037740707\n",
      "train accuracy: 1.0\n",
      "0.020737232640385628\n",
      "train accuracy: 0.97\n",
      "0.003446015529334545\n",
      "train accuracy: 1.0\n",
      "0.012072565034031868\n",
      "train accuracy: 0.98\n",
      "0.01045980304479599\n",
      "train accuracy: 0.99\n",
      "0.003196889301761985\n",
      "train accuracy: 1.0\n",
      "0.007867052219808102\n",
      "train accuracy: 1.0\n",
      "0.005742785055190325\n",
      "train accuracy: 0.99\n",
      "0.0015188998077064753\n",
      "train accuracy: 1.0\n",
      "0.008980738930404186\n",
      "train accuracy: 0.99\n",
      "0.003292081644758582\n",
      "train accuracy: 1.0\n",
      "0.004663859028369188\n",
      "train accuracy: 1.0\n",
      "0.0066046761348843575\n",
      "train accuracy: 1.0\n",
      "0.003179658902809024\n",
      "train accuracy: 1.0\n",
      "0.006242758594453335\n",
      "train accuracy: 1.0\n",
      "0.0032253353856503963\n",
      "train accuracy: 1.0\n",
      "0.0020234258845448494\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.97\n",
      "0.009860264870221727\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "mnist = Mnist(batch_size)\n",
    "print(batch_size)\n",
    "n_epochs = NUM_EPOCHS\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data,target) in enumerate(mnist.train_loader):\n",
    "        if USE_CUDA:\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = capsule_net(data)\n",
    "        loss = capsule_net.loss(data,output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if batch_id % 10 == 0:\n",
    "            output = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n",
    "            output = output.squeeze(-1)\n",
    "#             print(np.argmax(output.cpu().detach().numpy(),1))\n",
    "#             print(target.cpu().detach())\n",
    "            print(loss.item())\n",
    "            print(\"train accuracy:\",sum(np.argmax(output.cpu().detach().numpy(),1) == target.data.cpu().numpy())/float(batch_size))\n",
    "    \n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.test_loader):\n",
    "    #         labels = torch.sparse.torch.eye(10).index_select(dim=0, index=labels.data.cpu())\n",
    "\n",
    "            target = target.long()\n",
    "\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output = capsule_net(data)\n",
    "            loss = capsule_net.loss(data, output, target)\n",
    "\n",
    "    #         test_loss += loss.data[0]\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                output = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n",
    "                output = output.squeeze(-1)\n",
    "                print(\"test accuracy:\", sum(np.argmax(output.cpu().detach().numpy(),1) == target.data.cpu().numpy())/float(batch_size))\n",
    "\n",
    "    \n",
    "    print(test_loss / len(mnist.test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
