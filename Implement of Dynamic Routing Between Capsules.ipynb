{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms \n",
    "\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 3\n",
    "NUM_ROUTING_ITERATIONS = 3\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取mnist数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mnist:\n",
    "    def __init__(self, batch_size):\n",
    "        dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])\n",
    "\n",
    "        train_dataset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=dataset_transform)\n",
    "        test_dataset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=dataset_transform)\n",
    "        \n",
    "        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(v):\n",
    "    epsilon = 0.00000001\n",
    "    vector_norm = (v ** 2).sum(-1,keepdim = True) + epsilon\n",
    "    output = vector_norm * v/((1. + vector_norm) * torch.sqrt(vector_norm))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#第一层，使用普通卷积得到基础特征\n",
    "#(batch,28,28,1)\n",
    "class Convlayer(nn.Module):\n",
    "    def __init__(self,in_channels = 1,out_channels = 256,kernel_size = 9):\n",
    "        super(Convlayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size=kernel_size,\n",
    "                              stride = 1)\n",
    "    def forward(self,x):\n",
    "        #(batch_size,20,20,256)\n",
    "        return F.relu(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PrimaryCapslayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrimaryCapslayer(nn.Module):\n",
    "    def __init__(self,num_capsules=8, in_channels=256, out_channels=32, kernel_size=9):\n",
    "        super(PrimaryCapslayer,self).__init__()\n",
    "        self.capsules = nn.ModuleList([nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = kernel_size,stride = 2,padding=0)\n",
    "                                      for _ in range(num_capsules)])\n",
    "\n",
    "    def forward(self,x):\n",
    "        u = [capsule(x).view(x.size(0),-1,1) for capsule in self.capsules]\n",
    "        #u:(batch_size,8,6,6,32)\n",
    "        u = torch.cat(u,dim=-1)\n",
    "        #u:(batch_size,1152,8)\n",
    "        return squash(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DigitCaps and Dynamic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self,num_capsules = 10,num_routes = 32*6*6,in_channels = 8,out_channels = 16):\n",
    "        super(DigitCaps,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_capsules = num_capsules\n",
    "        self.num_routes = num_routes\n",
    "        \n",
    "        self.W = nn.Parameter(torch.randn(num_routes,num_capsules,out_channels,in_channels))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        u = x.unsqueeze(3)\n",
    "        u = u.unsqueeze(2)\n",
    "        #print(\"u.shape:\",u.shape)\n",
    "        #x:(batch_size,1152,1,8,1)\n",
    "        #W:(1152,10,16,8)\n",
    "        #W*x = (batch_size,1152,10,16,1)\n",
    "        #W = self.W.unsqueeze(0)\n",
    "        #print(\"w.shape\",self.W.shape)\n",
    "        u_hat = torch.matmul(self.W,u)\n",
    "        #u_hat(batch_size, 1152, 10, 16, 1)\n",
    "        u_hat = u_hat.squeeze(-1)\n",
    "        #print(\"u_hat.shape\",u_hat.shape)\n",
    "        u_hat=u_hat.permute(0,2,1,3)\n",
    "        \n",
    "        b_ij = torch.zeros(u_hat.size(0),self.num_capsules,1,self.num_routes)\n",
    "        if USE_CUDA:\n",
    "            b_ij = b_ij.cuda()\n",
    "            \n",
    "        num_iterations = NUM_ROUTING_ITERATIONS\n",
    "        for iteration in range(num_iterations):\n",
    "            #print(b_ij.shape)\n",
    "            c_ij = F.softmax(b_ij,-1)\n",
    "            #print(\"u_hat.shape\",u_hat.shape)\n",
    "            #print(\"c_ij.shape:\",c_ij.shape)\n",
    "            s_j = torch.matmul(c_ij,u_hat)\n",
    "            #print(\"s_j:\",s_j.shape)\n",
    "            v_j = squash(s_j)\n",
    "            if iteration < num_iterations -1:\n",
    "                a_ij = torch.matmul(v_j,u_hat.permute(0,1,3,2))\n",
    "                b_ij = b_ij + a_ij\n",
    "        v_j = v_j.permute(0,1,3,2).squeeze(-1)\n",
    "        #v_j :(batch,10,16)\n",
    "        return v_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x,data):\n",
    "        #x:(batch_size,10,16)\n",
    "        classes = torch.sqrt((x**2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        #classes(batch_size,10)\n",
    "        _,max_length_indices = classes.max(dim = 1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        if USE_CUDA:\n",
    "            masked = masked.cuda()\n",
    "        #print(max_length_indices.shape)\n",
    "        masked = masked.index_select(dim = 0,index=max_length_indices.data)\n",
    "        \n",
    "        masked = masked.unsqueeze(2)\n",
    "        #print(masked.shape)\n",
    "        #masked(100,10,1)\n",
    "        reconstractions = self.reconstraction_layers((masked*x).view(x.size(0), -1))\n",
    "        reconstractions = reconstractions.view(-1, 1, 28, 28)\n",
    "        \n",
    "        return reconstractions,masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet,self).__init__()\n",
    "        self.conv_layer = Convlayer()\n",
    "        self.primarycaps_layer = PrimaryCapslayer()\n",
    "        self.DigitCaps_layer = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self,data):\n",
    "        data = self.conv_layer(data)\n",
    "        data = self.primarycaps_layer(data)\n",
    "        output = self.DigitCaps_layer(data)\n",
    "        reconstractions,masked = self.decoder(output,data)\n",
    "        return output ,reconstractions,masked\n",
    "    \n",
    "    def loss(self, data, x, target,reconstractions):\n",
    "        loss1 = self.margin_loss(x, target)\n",
    "        loss2 = self.reconstruction_loss(data,reconstractions)\n",
    "        return  loss1,loss2\n",
    "    \n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2))\n",
    "        #print(\"vc\",v_c.shape)\n",
    "        #(batch_size,10)\n",
    "        left = F.relu(0.9 - v_c)**2\n",
    "        right = F.relu(v_c - 0.1)**2\n",
    "        labels = torch.sparse.torch.eye(10).index_select(dim=0, index=labels.data.cpu())\n",
    "        #print(labels.shape)\n",
    "        if USE_CUDA:\n",
    "            labels = labels.cuda()\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        #data(batch_size,1,28,28)\n",
    "        #reconstractions(batch_size,1,28,28)\n",
    "        loss = ((reconstructions.view(reconstructions.size(0), -1) - data.view(reconstructions.size(0), -1))**2).sum()/batch_size\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "capsule_net = CapsNet()\n",
    "if USE_CUDA:\n",
    "    capsule_net = capsule_net.cuda()\n",
    "optimizer = optim.Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haleypei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoc:0 [0/60000 (0%)]\tLoss1:0.804218 \tLoss2: 0.494283 \ttrain accuracy:0.1100\n",
      "Train Epoc:0 [1000/60000 (2%)]\tLoss1:0.258713 \tLoss2: 0.408298 \ttrain accuracy:0.7600\n",
      "Train Epoc:0 [2000/60000 (3%)]\tLoss1:0.179331 \tLoss2: 0.316093 \ttrain accuracy:0.8700\n",
      "Train Epoc:0 [3000/60000 (5%)]\tLoss1:0.150295 \tLoss2: 0.331899 \ttrain accuracy:0.8900\n",
      "Train Epoc:0 [4000/60000 (7%)]\tLoss1:0.104187 \tLoss2: 0.303006 \ttrain accuracy:0.8900\n",
      "Train Epoc:0 [5000/60000 (8%)]\tLoss1:0.082361 \tLoss2: 0.312958 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [6000/60000 (10%)]\tLoss1:0.092698 \tLoss2: 0.313303 \ttrain accuracy:0.9200\n",
      "Train Epoc:0 [7000/60000 (12%)]\tLoss1:0.048352 \tLoss2: 0.298465 \ttrain accuracy:0.9700\n",
      "Train Epoc:0 [8000/60000 (13%)]\tLoss1:0.063867 \tLoss2: 0.313722 \ttrain accuracy:0.9400\n",
      "Train Epoc:0 [9000/60000 (15%)]\tLoss1:0.031852 \tLoss2: 0.290149 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [10000/60000 (17%)]\tLoss1:0.065567 \tLoss2: 0.292407 \ttrain accuracy:0.9100\n",
      "Train Epoc:0 [11000/60000 (18%)]\tLoss1:0.035340 \tLoss2: 0.290778 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [12000/60000 (20%)]\tLoss1:0.050962 \tLoss2: 0.291867 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [13000/60000 (22%)]\tLoss1:0.044391 \tLoss2: 0.294890 \ttrain accuracy:0.9400\n",
      "Train Epoc:0 [14000/60000 (23%)]\tLoss1:0.053821 \tLoss2: 0.295541 \ttrain accuracy:0.9200\n",
      "Train Epoc:0 [15000/60000 (25%)]\tLoss1:0.023361 \tLoss2: 0.288607 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [16000/60000 (27%)]\tLoss1:0.038464 \tLoss2: 0.291847 \ttrain accuracy:0.9700\n",
      "Train Epoc:0 [17000/60000 (28%)]\tLoss1:0.027525 \tLoss2: 0.292510 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [18000/60000 (30%)]\tLoss1:0.021506 \tLoss2: 0.289496 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [19000/60000 (32%)]\tLoss1:0.039125 \tLoss2: 0.278503 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [20000/60000 (33%)]\tLoss1:0.016008 \tLoss2: 0.283596 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [21000/60000 (35%)]\tLoss1:0.025773 \tLoss2: 0.278460 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [22000/60000 (37%)]\tLoss1:0.020631 \tLoss2: 0.274988 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [23000/60000 (38%)]\tLoss1:0.030827 \tLoss2: 0.270756 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [24000/60000 (40%)]\tLoss1:0.030532 \tLoss2: 0.273174 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [25000/60000 (42%)]\tLoss1:0.019769 \tLoss2: 0.272121 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [26000/60000 (43%)]\tLoss1:0.026492 \tLoss2: 0.281088 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [27000/60000 (45%)]\tLoss1:0.011926 \tLoss2: 0.258393 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [28000/60000 (47%)]\tLoss1:0.026687 \tLoss2: 0.276470 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [29000/60000 (48%)]\tLoss1:0.012214 \tLoss2: 0.269498 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [30000/60000 (50%)]\tLoss1:0.008879 \tLoss2: 0.263270 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [31000/60000 (52%)]\tLoss1:0.022922 \tLoss2: 0.259298 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [32000/60000 (53%)]\tLoss1:0.011223 \tLoss2: 0.263999 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [33000/60000 (55%)]\tLoss1:0.013748 \tLoss2: 0.262916 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [34000/60000 (57%)]\tLoss1:0.019166 \tLoss2: 0.270019 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [35000/60000 (58%)]\tLoss1:0.015843 \tLoss2: 0.266999 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [36000/60000 (60%)]\tLoss1:0.023941 \tLoss2: 0.260111 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [37000/60000 (62%)]\tLoss1:0.026386 \tLoss2: 0.254035 \ttrain accuracy:0.9700\n",
      "Train Epoc:0 [38000/60000 (63%)]\tLoss1:0.007898 \tLoss2: 0.257672 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [39000/60000 (65%)]\tLoss1:0.013350 \tLoss2: 0.254101 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [40000/60000 (67%)]\tLoss1:0.017272 \tLoss2: 0.256604 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [41000/60000 (68%)]\tLoss1:0.037154 \tLoss2: 0.268812 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [42000/60000 (70%)]\tLoss1:0.011567 \tLoss2: 0.252868 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [43000/60000 (72%)]\tLoss1:0.014909 \tLoss2: 0.259189 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [44000/60000 (73%)]\tLoss1:0.023201 \tLoss2: 0.258058 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [45000/60000 (75%)]\tLoss1:0.005068 \tLoss2: 0.258293 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [46000/60000 (77%)]\tLoss1:0.018424 \tLoss2: 0.251892 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [47000/60000 (78%)]\tLoss1:0.020142 \tLoss2: 0.260055 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [48000/60000 (80%)]\tLoss1:0.009793 \tLoss2: 0.255994 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [49000/60000 (82%)]\tLoss1:0.020553 \tLoss2: 0.252889 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [50000/60000 (83%)]\tLoss1:0.021082 \tLoss2: 0.242037 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [51000/60000 (85%)]\tLoss1:0.010054 \tLoss2: 0.253761 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [52000/60000 (87%)]\tLoss1:0.012681 \tLoss2: 0.246875 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [53000/60000 (88%)]\tLoss1:0.034276 \tLoss2: 0.262139 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [54000/60000 (90%)]\tLoss1:0.011158 \tLoss2: 0.248664 \ttrain accuracy:1.0000\n",
      "Train Epoc:0 [55000/60000 (92%)]\tLoss1:0.030801 \tLoss2: 0.257692 \ttrain accuracy:0.9600\n",
      "Train Epoc:0 [56000/60000 (93%)]\tLoss1:0.010763 \tLoss2: 0.249554 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [57000/60000 (95%)]\tLoss1:0.016855 \tLoss2: 0.250173 \ttrain accuracy:0.9800\n",
      "Train Epoc:0 [58000/60000 (97%)]\tLoss1:0.011538 \tLoss2: 0.251977 \ttrain accuracy:0.9900\n",
      "Train Epoc:0 [59000/60000 (98%)]\tLoss1:0.017183 \tLoss2: 0.249750 \ttrain accuracy:0.9700\n",
      "test accuracy: 1.0\n",
      "0.26889438897371293\n",
      "Train Epoc:1 [0/60000 (0%)]\tLoss1:0.014482 \tLoss2: 0.255146 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [1000/60000 (2%)]\tLoss1:0.006526 \tLoss2: 0.249280 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [2000/60000 (3%)]\tLoss1:0.011755 \tLoss2: 0.250858 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [3000/60000 (5%)]\tLoss1:0.011226 \tLoss2: 0.252923 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [4000/60000 (7%)]\tLoss1:0.021882 \tLoss2: 0.255110 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [5000/60000 (8%)]\tLoss1:0.014303 \tLoss2: 0.245188 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [6000/60000 (10%)]\tLoss1:0.005861 \tLoss2: 0.249326 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [7000/60000 (12%)]\tLoss1:0.011228 \tLoss2: 0.254581 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [8000/60000 (13%)]\tLoss1:0.007176 \tLoss2: 0.254852 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [9000/60000 (15%)]\tLoss1:0.006089 \tLoss2: 0.262549 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [10000/60000 (17%)]\tLoss1:0.013619 \tLoss2: 0.241635 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [11000/60000 (18%)]\tLoss1:0.005935 \tLoss2: 0.245018 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [12000/60000 (20%)]\tLoss1:0.010840 \tLoss2: 0.250301 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [13000/60000 (22%)]\tLoss1:0.003323 \tLoss2: 0.248616 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [14000/60000 (23%)]\tLoss1:0.006152 \tLoss2: 0.252618 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [15000/60000 (25%)]\tLoss1:0.009200 \tLoss2: 0.241787 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [16000/60000 (27%)]\tLoss1:0.015719 \tLoss2: 0.246831 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [17000/60000 (28%)]\tLoss1:0.007206 \tLoss2: 0.252872 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [18000/60000 (30%)]\tLoss1:0.011880 \tLoss2: 0.256968 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [19000/60000 (32%)]\tLoss1:0.007298 \tLoss2: 0.251514 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [20000/60000 (33%)]\tLoss1:0.011074 \tLoss2: 0.253953 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [21000/60000 (35%)]\tLoss1:0.018861 \tLoss2: 0.253709 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [22000/60000 (37%)]\tLoss1:0.017535 \tLoss2: 0.232467 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [23000/60000 (38%)]\tLoss1:0.010146 \tLoss2: 0.245504 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [24000/60000 (40%)]\tLoss1:0.012496 \tLoss2: 0.238812 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [25000/60000 (42%)]\tLoss1:0.020187 \tLoss2: 0.242751 \ttrain accuracy:0.9600\n",
      "Train Epoc:1 [26000/60000 (43%)]\tLoss1:0.013656 \tLoss2: 0.239983 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [27000/60000 (45%)]\tLoss1:0.022041 \tLoss2: 0.242409 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [28000/60000 (47%)]\tLoss1:0.009171 \tLoss2: 0.260247 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [29000/60000 (48%)]\tLoss1:0.006772 \tLoss2: 0.251525 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [30000/60000 (50%)]\tLoss1:0.017714 \tLoss2: 0.248692 \ttrain accuracy:0.9700\n",
      "Train Epoc:1 [31000/60000 (52%)]\tLoss1:0.008761 \tLoss2: 0.245180 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [32000/60000 (53%)]\tLoss1:0.013314 \tLoss2: 0.245180 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [33000/60000 (55%)]\tLoss1:0.005154 \tLoss2: 0.243627 \ttrain accuracy:1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoc:1 [34000/60000 (57%)]\tLoss1:0.019103 \tLoss2: 0.238616 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [35000/60000 (58%)]\tLoss1:0.010566 \tLoss2: 0.246575 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [36000/60000 (60%)]\tLoss1:0.003144 \tLoss2: 0.250167 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [37000/60000 (62%)]\tLoss1:0.012458 \tLoss2: 0.244821 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [38000/60000 (63%)]\tLoss1:0.009910 \tLoss2: 0.244911 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [39000/60000 (65%)]\tLoss1:0.015046 \tLoss2: 0.233030 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [40000/60000 (67%)]\tLoss1:0.010226 \tLoss2: 0.239202 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [41000/60000 (68%)]\tLoss1:0.021918 \tLoss2: 0.242692 \ttrain accuracy:0.9700\n",
      "Train Epoc:1 [42000/60000 (70%)]\tLoss1:0.009420 \tLoss2: 0.239800 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [43000/60000 (72%)]\tLoss1:0.013459 \tLoss2: 0.246590 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [44000/60000 (73%)]\tLoss1:0.009029 \tLoss2: 0.243810 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [45000/60000 (75%)]\tLoss1:0.019724 \tLoss2: 0.244930 \ttrain accuracy:0.9700\n",
      "Train Epoc:1 [46000/60000 (77%)]\tLoss1:0.014534 \tLoss2: 0.250836 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [47000/60000 (78%)]\tLoss1:0.011414 \tLoss2: 0.239331 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [48000/60000 (80%)]\tLoss1:0.014496 \tLoss2: 0.241187 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [49000/60000 (82%)]\tLoss1:0.011927 \tLoss2: 0.242173 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [50000/60000 (83%)]\tLoss1:0.005295 \tLoss2: 0.240897 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [51000/60000 (85%)]\tLoss1:0.013174 \tLoss2: 0.248091 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [52000/60000 (87%)]\tLoss1:0.006338 \tLoss2: 0.237496 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [53000/60000 (88%)]\tLoss1:0.011197 \tLoss2: 0.244914 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [54000/60000 (90%)]\tLoss1:0.016836 \tLoss2: 0.240182 \ttrain accuracy:0.9800\n",
      "Train Epoc:1 [55000/60000 (92%)]\tLoss1:0.008363 \tLoss2: 0.249896 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [56000/60000 (93%)]\tLoss1:0.006385 \tLoss2: 0.235723 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [57000/60000 (95%)]\tLoss1:0.005624 \tLoss2: 0.241496 \ttrain accuracy:1.0000\n",
      "Train Epoc:1 [58000/60000 (97%)]\tLoss1:0.011939 \tLoss2: 0.239997 \ttrain accuracy:0.9900\n",
      "Train Epoc:1 [59000/60000 (98%)]\tLoss1:0.017754 \tLoss2: 0.239277 \ttrain accuracy:0.9900\n",
      "test accuracy: 1.0\n",
      "0.25394338876008987\n",
      "Train Epoc:2 [0/60000 (0%)]\tLoss1:0.001756 \tLoss2: 0.235814 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [1000/60000 (2%)]\tLoss1:0.006817 \tLoss2: 0.240245 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [2000/60000 (3%)]\tLoss1:0.003682 \tLoss2: 0.240850 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [3000/60000 (5%)]\tLoss1:0.010210 \tLoss2: 0.245166 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [4000/60000 (7%)]\tLoss1:0.011137 \tLoss2: 0.238410 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [5000/60000 (8%)]\tLoss1:0.006025 \tLoss2: 0.237527 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [6000/60000 (10%)]\tLoss1:0.005111 \tLoss2: 0.245323 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [7000/60000 (12%)]\tLoss1:0.011409 \tLoss2: 0.233143 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [8000/60000 (13%)]\tLoss1:0.001946 \tLoss2: 0.250140 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [9000/60000 (15%)]\tLoss1:0.001755 \tLoss2: 0.240612 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [10000/60000 (17%)]\tLoss1:0.004502 \tLoss2: 0.233414 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [11000/60000 (18%)]\tLoss1:0.007316 \tLoss2: 0.231994 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [12000/60000 (20%)]\tLoss1:0.005796 \tLoss2: 0.232210 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [13000/60000 (22%)]\tLoss1:0.010799 \tLoss2: 0.235175 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [14000/60000 (23%)]\tLoss1:0.002112 \tLoss2: 0.247804 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [15000/60000 (25%)]\tLoss1:0.012741 \tLoss2: 0.250801 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [16000/60000 (27%)]\tLoss1:0.007910 \tLoss2: 0.246194 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [17000/60000 (28%)]\tLoss1:0.002379 \tLoss2: 0.249095 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [18000/60000 (30%)]\tLoss1:0.014496 \tLoss2: 0.237813 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [19000/60000 (32%)]\tLoss1:0.002001 \tLoss2: 0.232218 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [20000/60000 (33%)]\tLoss1:0.003339 \tLoss2: 0.238342 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [21000/60000 (35%)]\tLoss1:0.002920 \tLoss2: 0.229267 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [22000/60000 (37%)]\tLoss1:0.008710 \tLoss2: 0.238259 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [23000/60000 (38%)]\tLoss1:0.014125 \tLoss2: 0.240070 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [24000/60000 (40%)]\tLoss1:0.009932 \tLoss2: 0.236070 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [25000/60000 (42%)]\tLoss1:0.011384 \tLoss2: 0.239803 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [26000/60000 (43%)]\tLoss1:0.005305 \tLoss2: 0.235541 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [27000/60000 (45%)]\tLoss1:0.003970 \tLoss2: 0.244287 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [28000/60000 (47%)]\tLoss1:0.005306 \tLoss2: 0.233161 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [29000/60000 (48%)]\tLoss1:0.007840 \tLoss2: 0.243664 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [30000/60000 (50%)]\tLoss1:0.007015 \tLoss2: 0.233781 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [31000/60000 (52%)]\tLoss1:0.005944 \tLoss2: 0.236851 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [32000/60000 (53%)]\tLoss1:0.005681 \tLoss2: 0.245069 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [33000/60000 (55%)]\tLoss1:0.004515 \tLoss2: 0.240644 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [34000/60000 (57%)]\tLoss1:0.001885 \tLoss2: 0.238512 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [35000/60000 (58%)]\tLoss1:0.006035 \tLoss2: 0.238443 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [36000/60000 (60%)]\tLoss1:0.012542 \tLoss2: 0.244585 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [37000/60000 (62%)]\tLoss1:0.001663 \tLoss2: 0.237429 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [38000/60000 (63%)]\tLoss1:0.004843 \tLoss2: 0.230799 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [39000/60000 (65%)]\tLoss1:0.008576 \tLoss2: 0.242748 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [40000/60000 (67%)]\tLoss1:0.006926 \tLoss2: 0.235201 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [41000/60000 (68%)]\tLoss1:0.011655 \tLoss2: 0.225273 \ttrain accuracy:0.9700\n",
      "Train Epoc:2 [42000/60000 (70%)]\tLoss1:0.011083 \tLoss2: 0.234033 \ttrain accuracy:0.9800\n",
      "Train Epoc:2 [43000/60000 (72%)]\tLoss1:0.016071 \tLoss2: 0.239977 \ttrain accuracy:0.9800\n",
      "Train Epoc:2 [44000/60000 (73%)]\tLoss1:0.004686 \tLoss2: 0.240647 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [45000/60000 (75%)]\tLoss1:0.013332 \tLoss2: 0.244500 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [46000/60000 (77%)]\tLoss1:0.011339 \tLoss2: 0.237490 \ttrain accuracy:0.9800\n",
      "Train Epoc:2 [47000/60000 (78%)]\tLoss1:0.007321 \tLoss2: 0.230127 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [48000/60000 (80%)]\tLoss1:0.003178 \tLoss2: 0.235462 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [49000/60000 (82%)]\tLoss1:0.004896 \tLoss2: 0.238998 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [50000/60000 (83%)]\tLoss1:0.006241 \tLoss2: 0.227879 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [51000/60000 (85%)]\tLoss1:0.004924 \tLoss2: 0.236633 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [52000/60000 (87%)]\tLoss1:0.005037 \tLoss2: 0.233855 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [53000/60000 (88%)]\tLoss1:0.008686 \tLoss2: 0.240740 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [54000/60000 (90%)]\tLoss1:0.014735 \tLoss2: 0.236520 \ttrain accuracy:0.9800\n",
      "Train Epoc:2 [55000/60000 (92%)]\tLoss1:0.004297 \tLoss2: 0.239354 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [56000/60000 (93%)]\tLoss1:0.007828 \tLoss2: 0.245242 \ttrain accuracy:0.9900\n",
      "Train Epoc:2 [57000/60000 (95%)]\tLoss1:0.009718 \tLoss2: 0.234610 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [58000/60000 (97%)]\tLoss1:0.003734 \tLoss2: 0.229218 \ttrain accuracy:1.0000\n",
      "Train Epoc:2 [59000/60000 (98%)]\tLoss1:0.002728 \tLoss2: 0.245650 \ttrain accuracy:1.0000\n",
      "test accuracy: 0.98\n",
      "0.24656420901417733\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "mnist = Mnist(batch_size)\n",
    "print(\"batch_size:\",batch_size)\n",
    "n_epochs = NUM_EPOCHS\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    for batch_id, (data,target) in enumerate(mnist.train_loader):\n",
    "        if USE_CUDA:\n",
    "            data,target = data.cuda(),target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output,reconstractions,masked = capsule_net(data)\n",
    "        loss1,loss2 = capsule_net.loss(data,output,target,reconstractions)\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        if batch_id % 10 == 0:\n",
    "            output = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n",
    "            output = output.squeeze(-1)\n",
    "#             print(np.argmax(output.cpu().detach().numpy(),1))\n",
    "#             print(target.cpu().detach())\n",
    "            acc = sum(np.argmax(output.cpu().detach().numpy(),1) == target.data.cpu().numpy())/float(batch_size)\n",
    "            print(\"Train Epoc:{} [{}/{} ({:.0f}%)]\\tLoss1:{:.6f} \\tLoss2: {:.6f} \\ttrain accuracy:{:.4f}\".format(\n",
    "            epoch,batch_id * batch_size,len(mnist.train_loader.dataset),100. * batch_id / len(mnist.train_loader),loss1.item(),loss2.item(),acc))\n",
    "\n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    for batch_id, (data, target) in enumerate(mnist.test_loader):\n",
    "    #         labels = torch.sparse.torch.eye(10).index_select(dim=0, index=labels.data.cpu())\n",
    "\n",
    "            target = target.long()\n",
    "\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            output,reconstractions,masked = capsule_net(data)\n",
    "            loss1,loss2 = capsule_net.loss(data,output,target,reconstractions)\n",
    "            loss = loss1 + loss2\n",
    "    #         test_loss += loss.data[0]\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                output = torch.sqrt((output**2).sum(dim=2, keepdim=True))\n",
    "                output = output.squeeze(-1)\n",
    "                print(\"test accuracy:\", sum(np.argmax(output.cpu().detach().numpy(),1) == target.data.cpu().numpy())/float(batch_size))\n",
    "\n",
    "    \n",
    "    print(test_loss / len(mnist.test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(capsule_net.state_dict(), 'params sqe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CapsNet(\n",
      "  (conv_layer): Convlayer(\n",
      "    (conv): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n",
      "  )\n",
      "  (primarycaps_layer): PrimaryCapslayer(\n",
      "    (capsules): ModuleList(\n",
      "      (0): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (1): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (2): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (3): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (4): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (5): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (6): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "      (7): Conv2d(256, 32, kernel_size=(9, 9), stride=(2, 2))\n",
      "    )\n",
      "  )\n",
      "  (DigitCaps_layer): DigitCaps()\n",
      "  (decoder): Decoder(\n",
      "    (reconstraction_layers): Sequential(\n",
      "      (0): Linear(in_features=160, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Linear(in_features=1024, out_features=784, bias=True)\n",
      "      (5): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (mse_loss): MSELoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model =  CapsNet()\n",
    "model.load_state_dict(torch.load('params sqe.pkl'))\n",
    "# if USE_CUDA:\n",
    "#     model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重构效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "test_iter = iter(mnist.test_loader)\n",
    "imgs,labels = next(test_iter)\n",
    "print(imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28]) torch.Size([50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haleypei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/haleypei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_test = torch.tensor(imgs[:50],dtype=torch.float)\n",
    "y_test = torch.tensor(labels[:50],dtype=torch.long)\n",
    "\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haleypei/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLkAAAFwCAYAAABU5KkWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9HYSVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1512x432 with 14 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "img_num = 7\n",
    "k = random.randint(0,30)\n",
    "\n",
    "imgs = x_test[k:img_num+k].cpu().numpy()\n",
    "label = y_test[k:img_num+k].cpu().numpy()\n",
    "\n",
    "y_true = torch.sparse.torch.eye(10).index_select(dim=0, index=y_test[k:img_num+k].cpu().data)\n",
    "# y_true = y_true.cuda()\n",
    "out,pred_imgs,masked = model(x_test[k:img_num+k])\n",
    "pred_label = torch.sqrt(torch.sum(out*out, 2))\n",
    "\n",
    "pred_imgs = pred_imgs.cpu().detach().numpy()\n",
    "pred_label = pred_label.cpu().detach().numpy()\n",
    "pred_label = np.argmax(pred_label,1)\n",
    "\n",
    "plt.figure(\"test Image\",figsize=(img_num*3,3*2))\n",
    "for i in range(img_num):\n",
    "    index = i\n",
    "    img = imgs[index]\n",
    "    title = label[index]\n",
    "    img = img.reshape([28, 28])\n",
    "    \n",
    "    plt.subplot(2,img_num,i+1)\n",
    "    plt.imshow(img,cmap=plt.cm.gray)\n",
    "    plt.axis('off') \n",
    "    plt.title(title)\n",
    "    \n",
    "for i in range(img_num):\n",
    "    index = i\n",
    "    img = pred_imgs[index]\n",
    "    plt.subplot(2,img_num,i+img_num+1)\n",
    "    \n",
    "    title = pred_label[index]\n",
    "    img = img.reshape([28, 28])\n",
    "    plt.imshow(img,cmap=plt.cm.gray)\n",
    "    plt.axis('off') \n",
    "    plt.title(title)\n",
    "    \n",
    "filename = 'r1'+str(time.time())+'.jpg'\n",
    "plt.savefig(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
